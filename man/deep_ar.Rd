% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/parsnip-deepar.R
\name{deep_ar}
\alias{deep_ar}
\title{General Interface for DeepAR Time Series Models}
\usage{
deep_ar(
  mode = "regression",
  id,
  freq,
  prediction_length,
  epochs = NULL,
  batch_size = NULL,
  num_batches_per_epoch = NULL,
  learn_rate = NULL,
  learn_rate_decay_factor = NULL,
  learn_rate_min = NULL,
  patience = NULL,
  clip_gradient = NULL,
  penalty = NULL,
  cell_type = NULL,
  num_layers = NULL,
  num_cells = NULL,
  dropout = NULL
)
}
\arguments{
\item{mode}{A single character string for the type of model.
The only possible value for this model is "regression".}

\item{id}{A quoted column name that tracks the GluonTS FieldName "item_id"}

\item{freq}{A \code{pandas} timeseries frequency such as "5min" for 5-minutes or "D" for daily.
Refer to \href{https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases}{Pandas Offset Aliases}.}

\item{prediction_length}{Numeric value indicating the length of the prediction horizon}

\item{epochs}{Number of epochs that the network will train (default: 100).}

\item{batch_size}{Number of examples in each batch (default: 32).}

\item{num_batches_per_epoch}{Number of batches at each epoch (default: 50).}

\item{learn_rate}{Initial learning rate (default: 10−3).}

\item{learn_rate_decay_factor}{Factor (between 0 and 1) by which to decrease the learning rate (default: 0.5).}

\item{learn_rate_min}{Lower bound for the learning rate (default: 5⋅10−5 ).}

\item{patience}{The patience to observe before reducing the learning rate, nonnegative integer (default: 10).}

\item{clip_gradient}{Maximum value of gradient. The gradient is clipped if it is too large (default: 10).}

\item{penalty}{The weight decay (or L2 regularization) coefficient. Modifies objective by adding a penalty for having large weights (default 10−8 ).}

\item{cell_type}{Type of recurrent cells to use (available: 'lstm' or 'gru'; default: 'lstm')}

\item{num_layers}{Number of RNN layers (default: 2)}

\item{num_cells}{Number of RNN cells for each layer (default: 40)}

\item{dropout}{Dropout regularization parameter (default: 0.1)}
}
\description{
\code{deep_ar()} is a way to generate a \emph{specification} of a DeepAR model
before fitting and allows the model to be created using
different packages. Currently the only package is \code{gluonts}.
}
\details{
These arguments are converted to their specific names at the time that
the model is fit. Other options and arguments can be set using
\code{set_engine()}. If left to their defaults here (see above),
the values are taken from the underlying model functions.
If parameters need to be modified, \code{update()} can be used in lieu of recreating
the object from scratch.

The model can be created using the fit() function using the following engines:
\itemize{
\item \strong{GluonTS:} "gluonts" (the default)
}
}
\section{Engine Details}{


The standardized parameter names in \code{modeltime} can be mapped to their original
names in each engine:\tabular{ll}{
   modeltime \tab DeepAREstimator \cr
   id \tab NA \cr
   freq \tab freq \cr
   prediction_length \tab prediction_length \cr
   epochs \tab epochs (20) \cr
   batch_size \tab batch_size (32) \cr
   num_batches_per_epoch \tab num_batches_per_epoch (4) \cr
   learn_rate \tab learning_rate (0.0001) \cr
   learn_rate_decay_factor \tab learning_rate_decay_factor (0.5) \cr
   learn_rate_min \tab minimum_learning_rate (5e-5) \cr
   patience \tab patience (10) \cr
   clip_gradient \tab clip_gradient (10) \cr
   penalty \tab weight_decay (1e-8) \cr
   cell_type \tab cell_type ('lstm') \cr
   num_layers \tab num_layers (2) \cr
   num_cells \tab num_cells (40) \cr
   dropout \tab dropout_rate (0.1) \cr
}


Other options can be set using \code{set_engine()}.

\strong{gluonts}

The engine uses \code{gluonts.model.deepar.DeepAREstimator()}.
Default values that have been changed to speed up computations
and increase stability:
\itemize{
\item \code{epochs = 20}: GluonTS uses 100 by default.
\item \code{num_batches_per_epoch = 4}: GluonTS uses 50 by default.
\item \code{learn_rate = 0.0001}: GluonTS uses 0.001 by default.
}

This implementation has several \emph{Required Parameters},
which are user defined.

\emph{ID Variable (Required):}

An important difference between other parsnip models is that
each time series (even single time series) must be uniquely identified
by an ID variable.
\itemize{
\item The ID feature must be of class \code{character} or \code{factor}.
\item This ID feature is provided as a quoted expression
during the model specification process (e.g. \code{deep_ar(id = "ID")} assuming
you have a column in your data named "ID").
}

\emph{Frequency (Required):}

The GluonTS models use a Pandas Timestamp Frequency \code{freq} to generate
features internally. Examples:
\itemize{
\item \verb{freq = "5min} for timestamps that are 5-minutes apart
\item \verb{freq = "D} for Daily Timestamps
}

The Pandas Timestamps are quite flexible.
Refer to \href{https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases}{Pandas Offset Aliases}.

\emph{Prediction Length (Required:}

Unlike other parsnip models, a \code{prediction_length} is required
during the model specification and fitting process.

\emph{Regressors (Not Used):}
\itemize{
\item The implementation is Univariate Only.
\item No external regressors are currently used.
}
}

\section{Fit Details}{


The following features are REQUIRED to be available in the incoming data for the
fitting process.

\strong{ID Variable}

An ID feature must be included in the recipe or formula fitting
process. The column name must match the quoted feature name specified in the
\code{deep_ar(id)} argument.

\strong{Date and Date-Time Variable}

It's a requirement to have a date or date-time variable as a predictor.
The \code{fit()} interface accepts date and date-time features and handles them internally.

\strong{Example}
\itemize{
\item \code{fit(y ~ date + id)}: Includes a target feature that is a
function of a date and id feature.
}
}

\examples{
library(tidymodels)
library(tidyverse)
library(timetk)


# ---- DEEP AR ----

# Model Spec
model_spec <- deep_ar(
    id                = "id",
    freq              = "M",
    prediction_length = 24,
    epochs            = 1,
    batch_size        = 4
) \%>\%
    set_engine("gluonts")

model_spec

# Trained Model
model_fitted <- model_spec \%>\%
    fit(value ~ date + id, m750)

model_fitted

}
\seealso{
\code{\link[=fit.model_spec]{fit.model_spec()}}, \code{\link[=set_engine]{set_engine()}}
}
